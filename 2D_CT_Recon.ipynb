{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae60ee04",
   "metadata": {},
   "source": [
    "# [Generalizable Neural Fields as Partially Observed Neural Processes](https://arxiv.org/abs/2309.06660)\n",
    "\n",
    "In this paper, we leverage neural processes (NPs) to both speed up neural field training and leverage our ability to train neural fields for related signals to condition the neural field for a signal of interest. Previous approaches used gradient-based meta-learning methods such as Reptile, but we show that NPs are superior to gradient-based meta-learning approaches. In this notebook, we replicate our experiment for 2D CT reconstruction. \n",
    "\n",
    "**Open notebook:** \n",
    "[![View on Github](https://img.shields.io/static/v1.svg?logo=github&label=Repo&message=View%20On%20Github&color=lightgrey)](https://github.com/its-gucci/partially-observed-neural-processes/blob/2D_CT_Recon.ipynb)\n",
    "[![Open In Collab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/its-gucci/partially-observed-neural-processes/blob/2D_CT_Recon.ipynb)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c223f2e2",
   "metadata": {},
   "source": [
    "Let's start by importing the libraries we'll need. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7823eb07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from map_coordinates import _map_coordinates\n",
    "\n",
    "import os\n",
    "os.chdir('neural_process_family')\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d511d03f",
   "metadata": {},
   "source": [
    "## Forward Map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e180fe4",
   "metadata": {},
   "source": [
    "For 2D CT reconstruction, we would like to reconstruct a 2D CT scan from 1D sensor observations called sinograms. The 1D sinograms are generated from the 2D CT scan through the following forward map (integral projection, adapted to PyTorch from [here](https://github.com/tancik/learnit/blob/main/Experiments/2d_ct.ipynb)). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3683360",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ct_project(img, theta, device='cuda'):\n",
    "    y, x = torch.meshgrid(torch.arange(img.shape[0])/img.shape[0] - 0.5,\n",
    "                          torch.arange(img.shape[0])/img.shape[0] - 0.5,\n",
    "                         )\n",
    "    x = x.to(device)\n",
    "    y = y.to(device)\n",
    "    x_rot = x*torch.cos(theta) - y*torch.sin(theta)\n",
    "    y_rot = x*torch.sin(theta) + y*torch.cos(theta)\n",
    "    x_rot = (x_rot + 0.5)*img.shape[1]\n",
    "    y_rot = (y_rot + 0.5)*img.shape[0]\n",
    "    sample_coords = torch.stack([y_rot, x_rot], dim=0)\n",
    "    resampled = _map_coordinates(img, sample_coords.to(device), 0, device=device).reshape(img.shape)\n",
    "    return resampled.mean(axis=0)[:,None,...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff9a6794",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ct_project_batch(img, thetas, device='cuda'):\n",
    "    projs = []\n",
    "    for theta in thetas:\n",
    "        projs.append(ct_project(img.squeeze(), theta, device=device))\n",
    "    return torch.stack(projs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b47cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ct_project_double_batch(imgs, thetas, device='cuda'):\n",
    "    '''\n",
    "    imgs: [batch, 256, 256]\n",
    "    thetas: [batch, n_projs]\n",
    "    '''\n",
    "    b = imgs.shape[0]\n",
    "    n_projs = thetas.shape[1]\n",
    "    projs = []\n",
    "    for i in range(b):\n",
    "        partial = []\n",
    "        for j in range(n_projs):\n",
    "            partial.append(ct_project(imgs[i], thetas[i][j], device=device))\n",
    "        projs.append(torch.stack(partial))\n",
    "    return torch.stack(projs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e646866b",
   "metadata": {},
   "source": [
    "## Architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05102629",
   "metadata": {},
   "source": [
    "We will implement our NP-based Partially-Observed Neural Process (PONP) with help from the [Neural Process Family library](https://github.com/YannDubs/Neural-Process-Family), which implements many different neural process algorithms. They also have a nice introduction to neural processes [here](https://yanndubs.github.io/Neural-Process-Family/text/Intro.html). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c062be4",
   "metadata": {},
   "source": [
    "### MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5eabef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from npf.architectures.mlp import MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a6a8c1a",
   "metadata": {},
   "source": [
    "### CT reconstruction model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "268ffd02",
   "metadata": {},
   "source": [
    "For fair comparison, we use the same CT reconstruction neural field as was used in previous work (adapted to PyTorch from [here](https://github.com/tancik/learnit/blob/main/Experiments/2d_ct.ipynb)). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8300d554",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CTReconModel(nn.Module):\n",
    "    def __init__(self, in_features=2, out_features=1, hidden_features=256, hidden_layers=5, device='cuda'):\n",
    "        super().__init__()\n",
    "        modules = []\n",
    "        \n",
    "        # first layer\n",
    "        modules.append(nn.Linear(hidden_features, hidden_features))\n",
    "        modules.append(nn.ReLU())\n",
    "        \n",
    "        # intermediate layers\n",
    "        for i in range(1, hidden_layers - 1):\n",
    "            modules.append(nn.Linear(hidden_features, hidden_features))\n",
    "            modules.append(nn.ReLU())\n",
    "            \n",
    "        # last layer\n",
    "        modules.append(nn.Linear(hidden_features, out_features))\n",
    "        modules.append(nn.ReLU())\n",
    "        \n",
    "        self.net = nn.Sequential(*modules)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b59fd4",
   "metadata": {},
   "source": [
    "### Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc08d18",
   "metadata": {},
   "source": [
    "We will use these networks to transform the output of our PONP decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6735c412",
   "metadata": {},
   "outputs": [],
   "source": [
    "class loc_transformer(nn.Module):\n",
    "    \n",
    "    def __init__(self, dim=256):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(dim, 1)\n",
    "        self.activation = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x, thetas):\n",
    "        x_shape = x.shape\n",
    "        x = self.linear(x).view(-1, 256, 256)\n",
    "        x = self.activation(x)\n",
    "        x = ct_project_double_batch(x, thetas.repeat(x_shape[0], 1))\n",
    "        return x.view(*x_shape[:-2], -1, 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f972e340",
   "metadata": {},
   "outputs": [],
   "source": [
    "class scale_transformer(nn.Module):\n",
    "    \n",
    "    def __init__(self, dim=256):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(dim, 1)\n",
    "        self.activation = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x, thetas):\n",
    "        x_shape = x.shape\n",
    "        x = self.linear(x).view(-1, 256, 256)\n",
    "        x = self.activation(x)\n",
    "        x = ct_project_double_batch(x, thetas.repeat(x_shape[0], 1))\n",
    "        x = 0.01 + 0.99 * F.softplus(x)\n",
    "        return x.view(*x_shape[:-2], -1, 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597e37e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class id_transformer(nn.Module):\n",
    "    \n",
    "    def __init__(self, transformer, stat='loc'):\n",
    "        super().__init__()\n",
    "        self.linear = transformer.linear\n",
    "        self.activation = transformer.activation\n",
    "        self.stat = stat\n",
    "    \n",
    "    def forward(self, x, _):\n",
    "        x_shape = x.shape\n",
    "        x = self.linear(x).view(*x_shape[:-2], 256, 256)\n",
    "        x = self.activation(x)\n",
    "        if self.stat == 'scale':\n",
    "            x = 0.01 + 0.99 * F.softplus(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6682466f",
   "metadata": {},
   "source": [
    "## Neural Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bab6a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "from npf import AttnLNP\n",
    "from npf.architectures import merge_flat_input\n",
    "from utils.helpers import count_parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a476125",
   "metadata": {},
   "source": [
    "### Adapt LNP to CT recon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ade8e7",
   "metadata": {},
   "source": [
    "We change the usual neural process model to incorporate training with our forward map. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "038cade5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttnLNPFM(AttnLNP):\n",
    "    \n",
    "    def __init__(self, x_dim, y_dim, **kwargs):\n",
    "        super().__init__(x_dim, y_dim, **kwargs)\n",
    "    \n",
    "    def forward(self, X_cntxt, Y_cntxt, X_trgt, Y_trgt=None, thetas=None):\n",
    "        try:\n",
    "            self.n_z_samples = (\n",
    "                self.n_z_samples_train.rvs()\n",
    "                if self.training\n",
    "                else self.n_z_samples_test.rvs()\n",
    "            )\n",
    "        except AttributeError:\n",
    "            self.n_z_samples = (\n",
    "                self.n_z_samples_train if self.training else self.n_z_samples_test\n",
    "            )\n",
    "        \n",
    "        # NeuralProcessFamily forward \n",
    "        self._validate_inputs(X_cntxt, Y_cntxt, X_trgt, Y_trgt)\n",
    "\n",
    "        # size = [batch_size, *n_cntxt, x_transf_dim]\n",
    "        X_cntxt = self.x_encoder(X_cntxt)\n",
    "        # size = [batch_size, *n_trgt, x_transf_dim]\n",
    "        X_trgt = self.x_encoder(X_trgt)\n",
    "\n",
    "        # {R^u}_u\n",
    "        # size = [batch_size, *n_rep, r_dim]\n",
    "        R = self.encode_globally(X_cntxt, Y_cntxt)\n",
    "\n",
    "        if self.encoded_path in [\"latent\", \"both\"]:\n",
    "            z_samples, q_zCc, q_zCct = self.latent_path(X_cntxt, R, X_cntxt, Y_trgt)\n",
    "        else:\n",
    "            z_samples, q_zCc, q_zCct = None, None, None\n",
    "\n",
    "        if self.encoded_path == \"latent\":\n",
    "            # if only latent path then cannot depend on deterministic representation\n",
    "            R = None\n",
    "\n",
    "        # size = [n_z_samples, batch_size, *n_trgt, r_dim]\n",
    "        R_trgt = self.trgt_dependent_representation(X_cntxt, z_samples, R, X_trgt)\n",
    "\n",
    "        # p(y|cntxt,trgt)\n",
    "        # batch shape=[n_z_samples, batch_size, *n_trgt] ; event shape=[y_dim]\n",
    "        p_yCc = self.decode(X_trgt, R_trgt, thetas)\n",
    "\n",
    "        return p_yCc, z_samples, q_zCc, q_zCct\n",
    "    \n",
    "    def decode(self, X_trgt, R_trgt, thetas):\n",
    "        \"\"\"\n",
    "        Compute predicted distribution conditioned on representation and\n",
    "        target positions.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X_trgt: torch.Tensor, size=[batch_size, *n_trgt, x_transf_dim]\n",
    "            Set of all target features {x^t}_t.\n",
    "        R_trgt : torch.Tensor, size=[n_z_samples, batch_size, *n_trgt, r_dim]\n",
    "            Set of all target representations {r^t}_t.\n",
    "        Return\n",
    "        ------\n",
    "        p_y_trgt: torch.distributions.Distribution, batch shape=[n_z_samples, batch_size, *n_trgt] ; event shape=[y_dim]\n",
    "            Posterior distribution for target values {p(Y^t|y_c; x_c, x_t)}_t\n",
    "        \"\"\"\n",
    "        # size = [n_z_samples, batch_size, *n_trgt, y_dim*2]\n",
    "        p_y_suffstat = self.decoder(X_trgt, R_trgt)\n",
    "\n",
    "        # size = [n_z_samples, batch_size, *n_trgt, y_dim]\n",
    "        p_y_loc, p_y_scale = p_y_suffstat.split(self.y_dim, dim=-1)\n",
    "\n",
    "        p_y_loc = self.p_y_loc_transformer(p_y_loc, thetas)\n",
    "        p_y_scale = self.p_y_scale_transformer(p_y_scale, thetas)\n",
    "\n",
    "        if not self.is_heteroskedastic:\n",
    "            n_z_samples, batch_size, *n_trgt, y_dim = p_y_scale.shape\n",
    "            p_y_scale = p_y_scale.view(n_z_samples * batch_size, *n_trgt, y_dim)\n",
    "            p_y_scale = pool_and_replicate_middle(p_y_scale)\n",
    "            p_y_scale = p_y_scale.view(n_z_samples, batch_size, *n_trgt, y_dim)\n",
    "\n",
    "        # batch shape=[n_z_samples, batch_size, *n_trgt] ; event shape=[y_dim]\n",
    "        p_yCc = self.PredictiveDistribution(p_y_loc, p_y_scale)\n",
    "\n",
    "        return p_yCc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0fefffc",
   "metadata": {},
   "source": [
    "### LNP settings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9192d1e8",
   "metadata": {},
   "source": [
    "Here, we set the hyperparameters and architecture for our PONP model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e34cc36",
   "metadata": {},
   "outputs": [],
   "source": [
    "R_DIM = 256\n",
    "KWARGS = dict(\n",
    "    is_q_zCct=False,  \n",
    "    n_z_samples_train=1,\n",
    "    n_z_samples_test=1,  \n",
    "    attention='transformer',\n",
    "    XEncoder=partial(MLP, n_hidden_layers=1, hidden_size=R_DIM),\n",
    "    Decoder=merge_flat_input(  # MLP takes single input but we give x and R so merge them\n",
    "        partial(CTReconModel, hidden_layers=4, hidden_features=256), is_sum_merge=True,\n",
    "    ),\n",
    "    r_dim=R_DIM,\n",
    "    p_y_loc_transformer=loc_transformer(),\n",
    "    p_y_scale_transformer=scale_transformer(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c8608ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1D case\n",
    "model_1d = partial(\n",
    "    AttnLNPFM,\n",
    "    x_dim=2,\n",
    "    y_dim=256,\n",
    "    XYEncoder=merge_flat_input(  # MLP takes single input but we give x and y so merge them\n",
    "        partial(MLP, n_hidden_layers=2, hidden_size=R_DIM * 2), is_sum_merge=True,\n",
    "    ),\n",
    "    **KWARGS,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f6da5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_params_1d = count_parameters(model_1d())\n",
    "print(f\"Number Parameters (1D): {n_params_1d:,d}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85896d57",
   "metadata": {},
   "source": [
    "## Dataset + Context/Target Getters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "161a6293",
   "metadata": {},
   "source": [
    "Here, we prepared the data for use in Neural Process Family framework. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d4d496",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f7478a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mgrid(sidelen, dim=2):\n",
    "    '''Generates a flattened grid of (x,y,...) coordinates in a range of -1 to 1.\n",
    "    sidelen: int\n",
    "    dim: int'''\n",
    "    tensors = tuple(dim * [torch.linspace(-1, 1, steps=sidelen)])\n",
    "    mgrid = torch.stack(torch.meshgrid(*tensors), dim=-1)\n",
    "    mgrid = mgrid.reshape(-1, dim)\n",
    "    return mgrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c91fd6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pi = torch.acos(torch.zeros(1)).item() * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffed1813",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'data/ct_256.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144578d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CTDataset(Dataset):\n",
    "    \n",
    "    shape=(1, 256, 256)\n",
    "    coords = get_mgrid(shape[1], dim=2)\n",
    "    name='ctrecon'\n",
    "    \n",
    "    def __init__(self, data_path=data_path, \n",
    "                 split='train', n_projs=20, transform=None, device='cuda'):\n",
    "        super(CTDataset, self).__init__()\n",
    "        with open(data_path, 'rb') as file:\n",
    "            dataset = pickle.load(file)\n",
    "        \n",
    "        if split == 'train':\n",
    "            self.data = dataset['data_train']\n",
    "        elif split == 'test':\n",
    "            self.data = dataset['data_test']\n",
    "            \n",
    "        self.transform = transform\n",
    "        self.n_projs = n_projs\n",
    "        self.device = device\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x = self.data[idx]\n",
    "        if self.transform:\n",
    "            x = self.transform(x)\n",
    "        thetas = pi * torch.rand(self.n_projs)\n",
    "        image_projs = ct_project_batch(x.squeeze(), thetas, device=self.device).squeeze()\n",
    "        return self.coords, image_projs, thetas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "270cbb6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ab0f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CTDataset(data_path, split='train', transform=transform, device='cpu')\n",
    "test_dataset = CTDataset(data_path, split='test', transform=transform, device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9805a19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ct_datasets = {'ctrecon': train_dataset}\n",
    "ct_test_datasets = {'ctrecon': test_dataset}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6662c621",
   "metadata": {},
   "source": [
    "### Collate functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21da2654",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neural_process_family.npf.utils.datasplit import CntxtTrgtGetter, GetRandomIndcs, get_all_indcs\n",
    "from utils.data import cntxt_trgt_collate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b25b8cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CntxtTrgtGetterCT(CntxtTrgtGetter):\n",
    "    \n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        \n",
    "    def __call__(\n",
    "        self, X, thetas, y=None, context_indcs=None, target_indcs=None, is_return_indcs=False\n",
    "    ):\n",
    "        batch_size, num_points_x = self.getter_inputs(X)\n",
    "        _, num_points_y = self.getter_inputs(y)\n",
    "\n",
    "        if context_indcs is None:\n",
    "            context_indcs = self.contexts_getter(batch_size, num_points_y)\n",
    "        if target_indcs is None:\n",
    "            target_indcs_x = self.targets_getter(batch_size, num_points_x)\n",
    "            target_indcs_y = self.targets_getter(batch_size, num_points_y)\n",
    "\n",
    "        if self.is_add_cntxts_to_trgts:\n",
    "            target_indcs_x = self.add_cntxts_to_trgts(\n",
    "                num_points_x, target_indcs_x, context_indcs_x\n",
    "            )\n",
    "            target_indcs_y = self.add_cntxts_to_trgts(\n",
    "                num_points_y, target_indcs_y, context_indcs_y\n",
    "            )\n",
    "\n",
    "        # only used if X for context and target should be different (besides selecting indices!)\n",
    "        X_pre_cntxt = self.preprocess_context(X)\n",
    "\n",
    "        if is_return_indcs:\n",
    "            # instead of features return indices / masks, and `Y_cntxt` is replaced\n",
    "            # with all values Y\n",
    "            return (\n",
    "                context_indcs,\n",
    "                X_pre_cntxt,\n",
    "                target_indcs,\n",
    "                X,\n",
    "            )\n",
    "\n",
    "        X_cntxt, Y_cntxt = self.select((thetas/pi).unsqueeze(-1).repeat(1, 1, 2), y, context_indcs)\n",
    "        X_trgt, Y_trgt = self.select(X, y, target_indcs_x, target_indcs_y)\n",
    "        return X_cntxt, Y_cntxt, X_trgt, Y_trgt, thetas\n",
    "    \n",
    "    def select(self, X, y, indcs, indcs_y=None):\n",
    "        \"\"\"Select the correct values from X.\"\"\"\n",
    "        batch_size, num_points, x_dim = X.shape\n",
    "        y_dim = y.size(-1)\n",
    "        indcs_x = indcs.to(X.device).unsqueeze(-1).expand(batch_size, -1, x_dim)\n",
    "        if indcs_y is not None:\n",
    "            indcs_y = indcs_y.to(X.device).unsqueeze(-1).expand(batch_size, -1, y_dim)\n",
    "        else:\n",
    "            indcs_y = indcs.to(X.device).unsqueeze(-1).expand(batch_size, -1, y_dim)\n",
    "        return (\n",
    "            torch.gather(X, 1, indcs_x).contiguous(),\n",
    "            torch.gather(y, 1, indcs_y).contiguous(),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac59e265",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cntxt_trgt_collate_ct(get_cntxt_trgt, is_duplicate_batch=False, **kwargs):\n",
    "    \"\"\"Transformes and collates inputs to neural processes given the whole input.\n",
    "    Parameters\n",
    "    ----------\n",
    "    get_cntxt_trgt : callable\n",
    "        Function that takes as input the features and tagrets `X`, `y` and return\n",
    "        the corresponding `X_cntxt, Y_cntxt, X_trgt, Y_trgt`.\n",
    "    is_duplicate_batch : bool, optional\n",
    "        Wether to repeat the batch to have 2 different context and target sets\n",
    "        for every function. If so the batch will contain the concatenation of both.\n",
    "    \"\"\"\n",
    "\n",
    "    def mycollate(batch):\n",
    "        collated = torch.utils.data.dataloader.default_collate(batch)\n",
    "        X = collated[0]\n",
    "        y = collated[1]\n",
    "        thetas = collated[2]\n",
    "\n",
    "        if is_duplicate_batch:\n",
    "            X = torch.cat([X, X], dim=0)\n",
    "            if y is not None:\n",
    "                y = torch.cat([y, y], dim=0)\n",
    "            y = torch.cat([y, y], dim=0)\n",
    "\n",
    "        X_cntxt, Y_cntxt, X_trgt, Y_trgt, thetas = get_cntxt_trgt(X, thetas, y, **kwargs)\n",
    "        inputs = dict(X_cntxt=X_cntxt, Y_cntxt=Y_cntxt, X_trgt=X_trgt, Y_trgt=Y_trgt, thetas=thetas)\n",
    "        targets = Y_trgt\n",
    "\n",
    "        return inputs, targets\n",
    "\n",
    "    return mycollate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb453d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "mycollate = cntxt_trgt_collate_ct(\n",
    "    CntxtTrgtGetterCT(\n",
    "        get_all_indcs,\n",
    "        get_all_indcs)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1112c49",
   "metadata": {},
   "source": [
    "## Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efdbb74e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import skorch\n",
    "from npf import ELBOLossLNPF, NLLLossLNPF\n",
    "from skorch.callbacks import GradientNormClipping, ProgressBar\n",
    "from utils.ntbks_helpers import add_y_dim\n",
    "from utils.train import train_models\n",
    "\n",
    "run = 1\n",
    "\n",
    "KWARGS = dict(\n",
    "    is_retrain=False,  # whether to load precomputed model or retrain\n",
    "    criterion=NLLLossLNPF,  # NPVI or NPML\n",
    "    chckpnt_dirname=\"saved_models\",\n",
    "    device=None,  # use GPU if available\n",
    "    batch_size=1,\n",
    "    lr=1e-4,\n",
    "    decay_lr=10,  # decrease learning rate by 10 during training\n",
    "    seed=None,\n",
    "    callbacks=[\n",
    "        GradientNormClipping(gradient_clip_value=1)\n",
    "    ],  # clipping gradients can stabilize training\n",
    "    starting_run=run,\n",
    "    runs=1,\n",
    ")\n",
    "\n",
    "\n",
    "# 1D\n",
    "trainers_1d = train_models(\n",
    "    ct_datasets,\n",
    "    {\"AttnLNP\": model_1d},\n",
    "    test_datasets=ct_test_datasets,\n",
    "    iterator_train__collate_fn=mycollate,\n",
    "    iterator_valid__collate_fn=mycollate,\n",
    "    max_epochs=200,\n",
    "    **KWARGS\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "772dec4a",
   "metadata": {},
   "source": [
    "## Test Time Optimization + Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03fc3bf4",
   "metadata": {},
   "source": [
    "Here, we perform test-time optimization as described in the paper. We also visualize the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59360426",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f3a1a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainers_1d['ctrecon/AttnLNP/run_{}'.format(r)].module_.p_y_loc_transformer = id_transformer(\n",
    "    trainers_1d['ctrecon/AttnLNP/run_{}'.format(r)].module_.p_y_loc_transformer, stat='loc'\n",
    ")\n",
    "trainers_1d['ctrecon/AttnLNP/run_{}'.format(r)].module_.p_y_scale_transformer = id_transformer(\n",
    "    trainers_1d['ctrecon/AttnLNP/run_{}'.format(r)].module_.p_y_scale_transformer, stat='scale'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd89a811",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(trainers_1d['ctrecon/AttnLNP/run_{}'.format(run)].module_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8007dc3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn off gradients for all layers except the CT recon model\n",
    "for name, param in trainers_1d['ctrecon/AttnLNP/run_{}'.format(run)].module_.named_parameters():\n",
    "    if name.startswith('decoder.flat_module') or name.startswith('p_y_'):\n",
    "        param.requires_grad = True\n",
    "    else:\n",
    "        param.requires_grad = False\n",
    "        \n",
    "    print(name, param.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e9ceb87",
   "metadata": {},
   "outputs": [],
   "source": [
    "coords = get_mgrid(256, dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1190793",
   "metadata": {},
   "outputs": [],
   "source": [
    "# views to test steps\n",
    "_views_to_test_steps = {\n",
    "    1: 50,\n",
    "    2: 100,\n",
    "    4: 1000,\n",
    "    8: 1000,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add87b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_psnr(model, views):\n",
    "    avg_mse = 0.0\n",
    "    avg_psnr = 0.0\n",
    "    # calculate thetas\n",
    "    thetas = torch.linspace(0, pi, views + 1)\n",
    "    thetas = thetas[:-1]\n",
    "    for i in range(len(test_dataset.data)):\n",
    "        # get test image and test projs\n",
    "        test_image = test_dataset.data[i]\n",
    "        image_projs = ct_project_batch(torch.from_numpy(test_image), thetas, device='cpu')\n",
    "        \n",
    "        # test time optimization\n",
    "        learner = copy.deepcopy(model).cuda()\n",
    "        opt = optim.Adam(learner.parameters(), lr=1e-4)\n",
    "        # for j in range(_views_to_test_steps[views]):\n",
    "        for j in range(0):\n",
    "            # pass through NP model to find pred\n",
    "            dist = learner.forward(\n",
    "                (thetas/pi).view(1, views, 1).repeat(1, 1, 2).cuda(),\n",
    "                image_projs.view(1, views, 256).cuda(),\n",
    "                coords.unsqueeze(0).cuda(),\n",
    "                image_projs.view(1, views, 256).cuda(),\n",
    "                thetas.cuda(),\n",
    "            )\n",
    "            preds = dist[0].base_dist.loc\n",
    "            # calculate predicted projections\n",
    "            pred_projs = ct_project_batch(preds.squeeze(), thetas)\n",
    "            \n",
    "            # calculate loss\n",
    "            loss = ((torch.from_numpy(test_image).cuda() - preds.squeeze())**2).mean()\n",
    "            \n",
    "            # update parameters\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step() \n",
    "        \n",
    "        # forward model to find NP pred\n",
    "        with torch.no_grad():\n",
    "            dist = learner.forward(\n",
    "                (thetas/pi).view(1, views, 1).repeat(1, 1, 2).cuda(),\n",
    "                image_projs.view(1, views, 256).cuda(),\n",
    "                coords.unsqueeze(0).cuda(),\n",
    "                image_projs.view(1, views, 256).cuda(),\n",
    "                thetas.cuda(),\n",
    "            )\n",
    "        preds = dist[0].base_dist.loc\n",
    "        print(preds.shape)\n",
    "        # calculate predicted projections\n",
    "        pred_projs = ct_project_batch(preds.squeeze(), thetas)\n",
    "        \n",
    "        # display predicted reconstruction and projection\n",
    "        plt.figure(figsize=(15,4))        \n",
    "        plt.subplot(1,2, 1)\n",
    "        plt.imshow(preds.reshape(256, 256).detach().cpu().numpy())\n",
    "        plt.title('Phantom')\n",
    "        plt.subplot(1,2, 2)\n",
    "        plt.imshow(pred_projs.detach().cpu().numpy()[:,:,0])\n",
    "        plt.title('Sinogram')\n",
    "        plt.show()\n",
    "            \n",
    "        # display groundtruth reconstruction and projection\n",
    "        plt.figure(figsize=(15,4))\n",
    "        plt.subplot(1,2, 1)\n",
    "        plt.imshow(test_image)\n",
    "        plt.title('Phantom')\n",
    "        plt.subplot(1,2, 2)\n",
    "        plt.imshow(image_projs.cpu().numpy()[:,:,0])\n",
    "        plt.title('Sinogram')\n",
    "        plt.show()\n",
    "        \n",
    "        loss = ((torch.from_numpy(test_image).cuda() - preds.squeeze())**2).mean()\n",
    "        psnr = -10 * torch.log10(loss)\n",
    "        print('MSE loss: {}'.format(loss))\n",
    "        print('PSNR: {}'.format(psnr))\n",
    "        \n",
    "        avg_mse += loss\n",
    "        avg_psnr += psnr\n",
    "        \n",
    "        print('Avg MSE loss: {}'.format(avg_mse/(i + 1)))\n",
    "        print('Avg PSNR: {}'.format(avg_psnr/(i + 1)))\n",
    "        \n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    print('Avg MSE loss: {}'.format(avg_mse/len(test_dataset.data)))\n",
    "    print('Avg PSNR: {}'.format(avg_psnr/len(test_dataset.data)))\n",
    "    return avg_mse/len(test_dataset.data), avg_psnr/len(test_dataset.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea20704",
   "metadata": {},
   "source": [
    "The number of input views used can be changed via the views argument. In the paper, we use views=1,2,4,8. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d22d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "find_psnr(trainers_1d['ctrecon/AttnLNP/run_{}'.format(run)].module_, views=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed14644",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
